TITLE: Random walk
AUTHOR: Torbj√∏rn Seland
DATE: today

TOC: on

===== Introduction =====
The last chapter will study a third way to model epidemic disease. This will be done by using random walk. This technique quite different from the other models presented earlier, by using Monte Carlo simulations and probabilities instead of differential equations, which have been in focus earlier. The first section will be a study of Monte Carlo methods and Random walk based on the paper from M.H. Jensen Ref. cite{hjorth2011computational}. The next sections will use the parameters from *English Boarding School* and *Walking Dead* to see if a Random walk system can expand the knowledge about epidemics. The model will be compared to the ODE system and PDE system from the previous chapters.   

===== Monte Carlo methods =====
Techniques from Monte Carlo are widely used in several fields as chemistry, physics, medicine, biology and in finance Ref. cite{hjorth2011computational}. These numerical methods can be seen in general terms as statistical simulations methods, which use random numbers to perform the simulations. The Metropolis algorithm is a central algorithm in this field, and is considered as one of the top ten algorithms during the last century Ref. cite{hjorth2011computational}. A Monte Carlo strategy require four terms to be understood to use this method. These are:
* Random variable
* probability distribution functions (PDF)
* moments of a PDE
* the pertinent variance $\sigma ^2$

=== Random variable ===
Random variable can be seen as stochastic variable, where the outcome cannot be presumed. Examples as tossing dice, flipping coins or gambling are based on this principle. Although the outcome is unknown, knowledge about the probability and the range can be studied. The numbers in the *domain* for two dice are
!bt
\begin{equation*}
{2,3,4,5,6,7,8,9,10,11,12}
\end{equation*}
!et
with the corresponding *probabilities* are
!bt
\begin{equation*}
{1,2,3,4,5,6,5,4,3,2,1}\frac{1}{36}
\end{equation*}
!et
By throwing two dice once, there is no guarantee that the result will be 7, though this has the highest probability. But by repeating this operation, the distribution would reflect the *probabilities* above. A stochastic variable can either be discrete or continuous, but will in both cases be denoted as capital letters, $X,Y$. A discrete example is the example above, where the domain is given with exact values,${x_1,x_2,x_3,...,x_n}$. The continuous case can be seen as the probability in a given area. An example can be the distance from a dart to the center, after a random thrown at a dartboard.

When using a computer to produce random number, they will in reality be pseudo random, since they have to be based on an algorithm. Therefore the choice of this algorithm is important. In this chapter, a generator of Marsaglia Ref. cite{marsaglia2003xorshift}

=== probability distribution functions (PDF) ===
The PDF is a function $p(x)$ on the domain that gives the probability or relative frequency for a outcome. In the discrete case, the function can be seen as
!bt
\begin{equation}
p(x) = Prob(X=x)
\end{equation}
!et
The PDF in the continuous is not able to directly depict the actual probability. The probability is instead defined as the density around $x$ with an infinitesimal interval. This can therefore be seen as an integral, since it is the density of the probability rather than the probability Ref.cite{hjorth2011computational}. This can be defined.
!bt
\begin{equation}
 Prob(a\leq X \leq b) = \int^b_a p(x)dx
\end{equation}
!et
And by quoting M.H. Jensen *Qualitatively speaking, a stochastic variable represents the values of numbers chosen as if by chance from some specified PDF so that the selection of a large set of these numbers reproduces this PDF.* Ref.cite{hjorth2011computational}. This sum up the relation between random variables and PDF. If this is not fulfilled, the group of stochastic variable does not fulfill the criteria for random numbers. 

CDF- *cumulative probability distribution function*

There are two properties that the PDF must fulfill. The first one is the size of $p(x)$. This has to be in the interval $0\geq p(x) \geq 1$, since the probability cannot be negative or larger than 1 for an event to happen. The sum of all events has to be 1, both for discrete and continuous PDFs, and can be seen as follows
!bt
\begin{equation}
	\begin{aligned}
    \sum_{x_i \in \mathbb{D}}\\
    \int_{x \in \mathbb{D}}
	\end{aligned}
\end{equation} 
!et

There are several distributions that are essential when looking at continuous PDFs. The two ones that will be used in this chapter are the uniform distribution.
!bt
\begin{equation} label{eq:uni_dist}
p(x) = \frac{1}{b-a}\theta(x-a)\theta(b-x)
\end{equation}
!et
with:
!bt
\begin{equation}
	\begin{aligned}
    \theta(x) = 0,\quad x < 0 \\
    \theta(x) = 1,\quad x \geq 0
	\end{aligned}
\end{equation}
!et
This distribution is natural to use, when a group of humans shall be evenly placed over an area. When comparing the ODE system from the first chapter and the uniformed distributed PDE system in previous chapter, Eq. (ref{eq:uni_dist}) is natural to use. To get a correct estimate, it is important that the set of random numbers is large enough. Gaussian distribution is the second one, this is often called normal distribution and can be seen in Eq.(ref{eq:gauss_dist})
!bt
\begin{equation} label{eq:gauss_dist}
	\begin{aligned}
    p(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp(-\frac{(x-\mu)^2}{2\sigma^2})
	\end{aligned}
\end{equation}
!et
This will give the same distribution as the Gaussian function used in the previous chapter. 


=== moments of a PDF ===
By define $h(x)$ as a arbitrary function, the *expectation value* can be written
!bt
\begin{equation}
    \langle h \rangle_X \equiv \int h(x)p(x)dx
\end{equation}
!et
Here defined on the domain of the stochastic variable $X$ with PDE $p(x)$. A more general way to write the expectation is by adding a power of,$n$, to the equation. This can now be seen as the *moments*. The $n$-th moment is defined
!bt
\begin{equation}
    \langle x^n \rangle \equiv \int x^np(x)dx
\end{equation}
!et
The value of $n$ can be sat to zero. This result in $\langle 1 \rangle$ and creates a normalization condition of $p$. The first order is called *mean* and are often defined with a $\mu$.
!bt
\begin{equation}
    \langle x \rangle = \mu \equiv \int xp(x)dx
\end{equation}
!et
This represents the average value of PDF and is often called the expectation value of $p$ Ref.cite{hjorth2011computational}.

=== The pertinent variance $\sigma ^2$ ===
*Central moments* is a special case of moments defined as
!bt
\begin{equation}
    \langle (x-\langle x \rangle)^n \rangle \equiv \int (x-\langle x \rangle)^np(x)dx
\end{equation}
!et
The first two central-moments are trivial and only result in 1 and 0, respectively for $n=0$ and $n=1$. But the second central-moment is more interesting to study. This is denoted as $\sigma^2_X$ or Var(X), called the variance. This can be shown.
!bt
\begin{equation}
    \sigma^2_X  = \langle x^2\rangle -\langle x \rangle^2 
\end{equation}
!et
The square root of the variance, $sigma = \sqrt{\langle (x-\langle x \rangle)^2 \rangle}$ is called *standard deviation*. This is the deviation from the mean of PDF, and can be seen as the spread around the mean of PDF.

===== Random walks =====
The previous section explained the technical aspects of Monte Carlo, as the principle and algorithms for producing random numbers. This section will study the importance of a proper selection of variables and importance sampling. The challenge when model a Monte Carlo simulation, is the appropriate selection of random states. It is important that this match the probability distribution, PDF. This will be done through a Markov process, which is a random walk with a selected probability for making a move. A good reason to choose Markov process, is that this will reach equilibrium state, after certain number of simulations. This can first be shown for a simple diffusion equation, which can be expanded to the simple PDE system used in the previous chapter.

=== Diffusion equation and random walks ===
The British Botanist R. Brown developed a theory by studying how pollen dispersed in water. This idea is called Brownian motion and can be used to describe a group of particles spreading. The function $w(x,t)dx$ can be defined as the probability of finding a given number of particles in an interval with size $dx$. This function is the $PDF$ as explained in the section above. The flux of particles that passing a point $x$, can be described by $j(x,t). This flow is proportionally with the gradient of the $PDF$,
!bt
\begin{equation}
j(x,t) = -D\frac{\partial w(x,t)}{\partial x}
\end{equation}
!et
$D$ is here the diffusion constant. This example can be seen as a closed experiment, where the concentration is conserved. The relation between the flux and the $PDF$ can also be expressed
!bt
\begin{equation}
\frac{\partial j(x,t)}{\partial x} = -\frac{\partial w(x,t)}{\partial t} 
\end{equation}
!et
And a diffusion equation can be expressed based on these two equations.
!bt
\begin{equation}
\frac{\partial w(x,t)}{\partial t} = D\frac{\partial^2 w(x,t)}{\partial x^2} 
\end{equation}
!et
The expectation value and the variance, $\sigma^2$, can be study. The expectation value for the function $f(x,t)$ can be defined as
!bt
\begin{equation}
\langle f(x,t) \rangle = \int^{\infty}_{-\infty} f(x,t) w(x,t)dx
\end{equation}
!et
The demands defined for the PDF in the previous section have to be fulfilled for $w(x,t)$. The normalization condition which is defined for this PDF require constraints for equation.
!bt
\begin{equation} label{eq:PDF_demands}
w(x= \pm \infty,t) = 0,\quad \frac{\partial^nw(x,t)}{\partial x^n}|_{x=\pm\infty} = 0 
\end{equation}
!et
This can be used to study the time derivative of the expectation value. The diffusion equation derived earlier can be used here.
!bt
\begin{equation}
\frac{\partial \langle f(x,t) \rangle}{\partial t} = \int^{\infty}_{-\infty} f(x,t) \frac{\partial w(x,t)}{\partial t}dx = D \int^{\infty}_{-\infty} f(x,t) \frac{\partial^2 w(x,t)}{\partial x^2}dx
\end{equation}
!et
By using integration by parts on the right side, the equation can be expressed,
!bt
\begin{equation} label{eq:disp_0}
\frac{\partial \langle f(x,t) \rangle}{\partial t} = D f(x,t) \frac{\partial w(x,t)}{\partial x}|_{x=\pm \infty} - 2D \int^{\infty}_{-\infty} \frac{\partial w(x,t)}{\partial x}dx
\end{equation}
!et
By using the demands from (ref{eq:PDF_demands}), the $\langle f(x,t) \rangle$ is independent of time. 
!bt
\begin{equation}
\frac{\partial \langle f(x,t) \rangle}{\partial t} = 0
\end{equation}
!et
Implying that if a simulation of a random walk with equal probability of jumping in each direction, will end up with the probability distribution centered around the initial position. This may not be the case for the variance. By using (ref{eq:disp_0}), the $\langle x^2 \rangle $ can be found. 
!bt
\begin{equation} 
\frac{\partial \langle x^2 \rangle}{\partial t} = D x^2 \frac{\partial w(x,t)}{\partial x}|_{x=\pm \infty} - 2D \int^{\infty}_{-\infty} x\frac{\partial w(x,t)}{\partial x}dx
\end{equation}
!et
Here, integration by parts can be used. This results in
!bt
\begin{equation}
\frac{\partial \langle x^2 \rangle}{\partial t} = D x w(x,t)|_{x=\pm \infty} + 2D \int^{\infty}_{-\infty} w(x,t)dx = 2D
\end{equation}
!et
this leads to
!bt
\begin{equation}
\langle x^2 \rangle = 2Dt
\end{equation}
!et
which gives the variance
!bt
\begin{equation} label{eq:diff_var}
\langle x^2 \rangle -\langle x \rangle^2 = 2Dt
\end{equation}
!et
And the square root of the variance can be expressed,
!bt
\begin{equation}
\sqrt{\langle x^2 \rangle -\langle x \rangle^2} = \sqrt{2Dt}
\end{equation}
!et
By comparing this with the displacement of a free particle, which moves with the function $x(t)=vt$ from the initial point, will the diffusion process moves with $\sqrt{\langle x^2 \rangle -\langle x \rangle^2} \propto \sqrt{t}$. This can be used to describe a random walk, and could say that a random walker escapes much more slowly than a free particle from the initial time. This can be seen in an example. 

=== Random walker ===
Now the random walker can be introduced in 1D. This can either jump to the left or the right with a lenght $\Delta x = l$. It is equal probability for both directions. $L=R=1/2$. Then the average displacement will be 
!bt
\begin{equation}
 \langle x(n) \rangle = \sum^n_i \Delta x_i = 0, \quad \Delta x_i = \pm l,
\end{equation}
!et
after $n$ jumps. The variance can be found by first finding $\langle x(n)^2 \rangle$.
!bt
\begin{equation}
\langle x(n)^2 \rangle = \left(\sum^n_i \Delta x_i\right)\left(\sum^n_j \Delta x_j\right) = \sum^n_i \Delta x_i^2 + \sum^n_i \Delta x_i \Delta x_j = l^2n
\end{equation}
!et
The last term here will disappear after enough steps.  
!bt
\begin{equation}
\sum^n_i \Delta x_i \Delta x_j = 0
\end{equation}
!et
This gives the variance
!bt
\begin{equation}
\langle x(n)^2 \rangle-\langle x(n) \rangle^2 = l^2n
\end{equation}
!et
Now this variance from a random walker can be coupled with the variance from the diffusion equation in the section above. By setting $n = t/\Delta t$. The random walker gets the following variance
!bt
\begin{equation}
\langle x(n)^2 \rangle-\langle x(n) \rangle^2 = l^2\frac{t}{\Delta t}
\end{equation}
!et
Then the diffusion constant in (ref{eq:diff_var}) can be replaced by
!bt
\begin{equation}
D = \frac{l^2}{\Delta t}
\end{equation}
!et
And the variance between these can be compared.

FIGURE:[plots/random_compare.png, height=500 width=800 frac=0.8] label{fig:gauss_random} 10000 random walkers placed at x=0 at t=0. Every step has the length $\Delta x = 0.01$ and with a random step every $\Delta t = 0.01$.

The standard deviation can be found for this simulation. This is given by the square root of the variance
!bt
\begin{equation}
\sigma = \sqrt{l^2\frac{t}{\Delta t}}
\end{equation}
!et

To study if the average random walker develops as expected, a table can with the outputs can be added. 

label{table:gauss_random}
|-----------------------------------------------|
|-----------------------|time=2     |time=8     |
|-----------------------------------------------|
|average displacement   |0.000662   | -0.002416 |
|standard deviation(SD) |0.1414     |  0.2828   |
|percent inside SD      |70.96 %    |69.02 %    |
|percent inside gauss   |68.26 %    |68.26 %    |
|-----------------------------------------------|

The average displacement and the standard deviation shows that a group of random walkes spread similar as a standard diffusion function. This result in a major group of random walkers. In the simulation in Fig.(ref{fig:gauss_random}), 10000 random walkers are used. By increasing the amount om random walkers, the precision will be better.

===== Epidemic in an English Boarding School =====
This example has been common for all three systems, and will be used in this chapter. The following system(ref{eq:simple_PDE}) from previous chapter, *Geographic models* will be implemented for random walkers to simulate the epidemic in the English Boarding School.
!bt
\begin{equation} label{eq:simple_PDE}
	\begin{aligned}
	\frac{\partial S}{\partial t} &= -rIS + D\nabla ^2 S\\
	\frac{\partial I}{\partial t} &= rIS- aI + D\nabla ^2 I\\
	\frac{\partial R}{\partial t} &= aI + D\nabla ^2 R
	\end{aligned}
\end{equation}
!et

In previous section, the diffusion term was








======= Bibliography =======

BIBFILE: ../bibliography/papers.pub
